<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home Page on Temporal keyword analysis</title><link>https://4ug-aug.github.io/ComSocWeb/</link><description>Recent content in Home Page on Temporal keyword analysis</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://4ug-aug.github.io/ComSocWeb/index.xml" rel="self" type="application/rss+xml"/><item><title>Data description</title><link>https://4ug-aug.github.io/ComSocWeb/data-description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://4ug-aug.github.io/ComSocWeb/data-description/</guid><description>Understanding the dataset Now that we have our raw dataset, let&amp;rsquo;s investigate and clean it based on our findings before moving on with further analysis.
For readability we briefly present the structure of this section. We start by inspecting our collected data as a whole, examining the number of papers, referenced papers, and potential data issues regarding missing values. From these findings we motivate our choice of data cleaning. We then move on and examine the cleaned dataset grouped by year, to examine the number of papers and references within these.</description></item><item><title>Network analysis</title><link>https://4ug-aug.github.io/ComSocWeb/network-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://4ug-aug.github.io/ComSocWeb/network-analysis/</guid><description>Initial Graph Analysis Now that we have presented our data, cleaned it we move on and create graph representations for each rolling window. Following this we provide a brief preliminary analysis of the graphs before moving on with the main analysis.
Let&amp;rsquo;s build some graphs! :-) The graphs are constructed for each decade, defined as a 10 year interval. The nodes are the papers and the edges are the references between them.</description></item><item><title>Text analysis</title><link>https://4ug-aug.github.io/ComSocWeb/text-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://4ug-aug.github.io/ComSocWeb/text-analysis/</guid><description>With the graphs and communities created and analysed, we can move on to analysing the abstracts of each community. We will do this by using the TF-IDF method to find the most important words in each community. To use this method we need all abstracts needs to be tokenised. The tokenised is based on the NLTK word_tokenizer, which essentially just splits the document based on white space. We can then import a dicitionary of stop words, and define our own filter on characters to further filter the tokens.</description></item></channel></rss>